\subsection{Linear Algebra}

A \textbf{vector space} over a field $F$ is a set $V$ with two operations, addition $+: V \times V \to V$ and scalar multiplication $\cdot: F \times V \to V$, that satisfy the following axioms.
\begin{subequations}
\begin{align}
    \forall u, v, w \in V: \qquad & u + (v + w) = (u + v) + w & \text{associativity of vector addition} \\
    \forall u, v \in V: \qquad & u + v = v + u & \text{commutativity of vector addition} \\
    \exists 0_V \in V\ \forall v \in V: \qquad & v + 0_V = v & \text{identity element of vector addition} \\
    \forall v \in V\ \exists -v \in V: \qquad & -v + v = 0_V & \text{inverse element of vector addition} \\
    \forall v \in V\ a, b \in F: \qquad & a \cdot (b \cdot v) = (a \cdot b) \cdot v & \text{compatibility of multiplications\footnotemark} \\
    \forall v \in V: \qquad & 1 \cdot v = v & \text{identity element of scalar multiplication} \\
    \forall u, v \in V\ a \in F: \qquad & a \cdot (u + v) = a \cdot u + a \cdot v & \text{distributivity of scalar multiplication\footnotemark} \\
    \forall v \in V\ a, b \in F: \qquad & (a + b) \cdot v = a \cdot v + b \cdot v & \text{distributivity of scalar multiplication\footnotemark}
\end{align}
\end{subequations}
\footnotetext{compatibility of scalar multiplication and field multiplication}
\footnotetext{with respect to vector addition}
\footnotetext{with respect to field addition}

A \textbf{linear map} $m$ between vector spaces $V$ and $W$ is a map $m: V \to W$ that satisfies the following two conditions:
\begin{subequations}
\begin{align}
    \forall u, v \in V: \qquad & m(u + v) = m(u) + m(v) & \text{additivity} \\
    \forall c \in \C, v \in V: \qquad & m(c \cdot v) = c \cdot m(v) & \text{homogeneity}
\end{align}
\end {subequations}
In our case both vector spaces $V$ and $W$ always have the same, so $V = W$.
The subset of all \textbf{invertible} linear maps between $V$ and itself is denoted as $\GL(V)$.
\\

In most cases, the representations of groups are over finite-dimensional vector spaces.
If they are finite-dimensional, they will be vector spaces over the field of complex numbers $\C$ in this document.
A $n$-dimensional complex vector space is denoted as $\C^n$.

\todo[inline]{add kernel, image, surjective, injective}

If a linear map is between two finite-dimensional vector spaces and both of those have a defined basis, it can be represented by a \textbf{matrix}.
In our finite-dimensional cases, both vector spaces are $\C^n$ and we define the standard basis $\{e_i | 0 \leq i < n\}$ where $e_i = \begin{pmatrix} 0 & \ldots & 0 & 1 & 0 & \ldots & 0 \end{pmatrix}^T$ is a vector with all zeros except the $i$-th element.
So we can represent the linear maps as matrices in the finite-dimensional cases in this paper.
\textbf{Matrix multiplication} $\cdot: \C^{a \times b} \times \C^{b \times c} \to \C^{a \times c}$ is equivalent to the composition of the linear maps, they represent.
It is defined as
\begin{align}
    A \cdot B = \left(\sum_k a_{ik} \cdot b_{kj}\right)_{ij}
\end{align}
A matrix $A \in \C^{n \times n}$ is \textbf{invertible}, if and only if it represents an invertible linear map.
This is the case if and only if there exists a matrix $A^{-1} \in C^{n \times n}$, such that $A^{-1} \cdot A = I$, where $I$ is the identity matrix. 
The identity matrix is defined as $I = (\delta_{kj})_{kj}$ where $\delta_{kj}$ is the Kronecker delta defined as
\begin{align}
    \delta_{kj} = \begin{cases}
        1 & \text{, if } k = j \\
        0 & \text{, else}
    \end{cases}
\end{align}
\todo[inline]{bijective}

A linear map from a complex vector space to itself is represented as a \textbf{square} matrix.
The following two concepts only exist for this kind of map.
The \textbf{trace} of a square matrix $\Tr: \C^{n \times n} \to \C$ is defined as
\begin{align}
    \Tr(A) = \sum_i a_{ii}
\end{align}

For a linear map $A$ from a complex vector space $\C^n$ to itself, there is at least one vector $v \in \C^n$ and a scalar $\lambda \in \C$ so that
\begin{align}
    A \cdot v & = \lambda \cdot v
\end{align}
$v$ is called \textbf{eigenvector} of $A$ and $\lambda$ is called \textbf{eigenvalue} of $A$.
The set of all eigenvalues of a matrix is denoted $\sigma_A = \left\{ \lambda_i \right\}$.
If $v_i \in \C^n$ is an eigenvector with the eigenvalue $\lambda_i$, then every multiple $\kappa \cdot v_i$ with $\kappa \in \C$ is also an eigenvector of $A$ with the eigenvalue $\lambda_i$.
There may be other vectors that also satisfy the condition for the same $\lambda_i$.
All the vectors, that do form a vector subspace denoted
\begin{align}
    E_{\lambda_i} & = \left\{ v \in \C^n | A \cdot v = \lambda_i \cdot v \right\}
\end{align}

Some properties of eigenvalues, we need later, are the following.
If the matrix $A$ is of finite order $n$, then its eigenvalues must be $n$-th roots of unity $\lambda_i^n = 1$.
The sum of the eigenvalues of a matrix $\sum_{\lambda_i \in \sigma_A} \lambda_i$ is identical to its trace $Tr(A)$.

\todo[inline]{direct sum of vector spaces}
\todo[inline]{add complementary vector spaces}

The \textbf{direct sum} of two matrices $\oplus: \C^{a \times b} \times \C^{c \times d} \to \C^{(a + c) \times (b + d)}$ is defined as
\begin{align}
    A \oplus B = \begin{pmatrix}
        A & 0 \\
        0 & B
    \end{pmatrix}
\end{align}
where each zero stands for a matrix that only has zeros.
Matrices, that can be written as the direct sum of two or more matrices are called \textbf{block matrices}.
\todo[inline]{trace of direct sum}

\todo[inline]{look at it again}
The \textbf{tensor product} of two matrices $\otimes: \C^{a \times b} \times \C^{c \times d} \to \C^{(a \cdot c) \times (b \cdot d)}$ is defined as
\begin{align}
    A \otimes B = \begin{pmatrix}
        a_{1, 1} \cdot B & a_{1, 2} \cdot B & \ldots & a_{1, b} \cdot B \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{a, 1} \cdot B & a_{a, 2} \cdot B & \ldots & a_{a, b} \cdot B
    \end{pmatrix}
\end{align}
