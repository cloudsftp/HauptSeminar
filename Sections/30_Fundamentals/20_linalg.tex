\subsection{Linear Algebra}

A \textbf{vector space} over a field $F$ is a set $V$ with two operations, addition $+: V \times V \to V$ and scalar multiplication $\cdot: F \times V \to V$, that satisfy the following axioms.
\begin{subequations}
\begin{align}
    \forall u, v, w \in V: \qquad & u + (v + w) = (u + v) + w & \text{associativity of vector addition} \\
    \forall u, v \in V: \qquad & u + v = v + u & \text{commutativity of vector addition} \\
    \exists 0_V \in V\ \forall v \in V: \qquad & v + 0_V = v & \text{identity element of vector addition} \\
    \forall v \in V\ \exists -v \in V: \qquad & -v + v = 0_V & \text{inverse element of vector addition} \\
    \forall v \in V\ a, b \in F: \qquad & a \cdot (b \cdot v) = (a \cdot b) \cdot v & \text{compatibility of multiplications\footnotemark} \\
    \forall v \in V: \qquad & 1 \cdot v = v & \text{identity element of scalar multiplication} \\
    \forall u, v \in V\ a \in F: \qquad & a \cdot (u + v) = a \cdot u + a \cdot v & \text{distributivity of scalar multiplication\footnotemark} \\
    \forall v \in V\ a, b \in F: \qquad & (a + b) \cdot v = a \cdot v + b \cdot v & \text{distributivity of scalar multiplication\footnotemark}
\end{align}
\end{subequations}
\footnotetext{compatibility of scalar multiplication and field multiplication}
\footnotetext{with respect to vector addition}
\footnotetext{with respect to field addition}

A \textbf{linear map} $m$ between vector spaces $V$ and $W$ is a map $m: V \to W$ that satisfies the following two conditions:
\begin{subequations}
\begin{align}
    \forall u, v \in V: \qquad & m(u + v) = m(u) + m(v) & \text{additivity} \\
    \forall c \in \C, v \in V: \qquad & m(c \cdot v) = c \cdot m(v) & \text{homogeneity}
\end{align}
\end {subequations}
In our case both vector spaces $V$ and $W$ always have the same, so $V = W$.
The subset of all \textbf{invertible} linear maps between $V$ and itself is denoted as $\GL(V)$.
\\

In most cases, the representations of groups are over finite-dimensional vector spaces.
If they are finite-dimensional, they will be vector spaces over the field of complex numbers $\C$ in this work.
A $n$-dimensional complex vector space is denoted as $\C^n$.

If a linear map is between two finite-dimensional vector spaces and both of those have a defined basis, it can be represented by a \textbf{matrix}.
In our finite-dimensional cases, both vector spaces are $\C^n$ and we define the standard basis $\{e_i | 0 \leq i < n\}$ where $e_i = \begin{pmatrix} 0 & \ldots & 0 & 1 & 0 & \ldots & 0 \end{pmatrix}^T$ is a vector with all zeros except the $i$-th element.
So we can represent the linear maps as matrices in the finite-dimensional cases in this paper.
\textbf{Matrix multiplication} $\cdot: \C^{a \times b} \times \C^{b \times c} \to \C^{a \times c}$ is equivalent to the composition of the linear maps, they represent.
It is defined as
\begin{align}
    A \cdot B = \left(\sum_k a_{ik} \cdot b_{kj}\right)_{ij}
\end{align}
A matrix $A \in \C^{n \times n}$ is \textbf{invertible}, if and only if it represents an invertible linear map.
This is the case if and only if there exists a matrix $A^{-1} \in C^{n \times n}$, such that $A^{-1} \cdot A = I$, where $I$ is the identity matrix. 
The identity matrix is defined as $I = (\delta_{kj})_{kj}$ where $\delta_{kj}$ is the Kronecker delta defined as
\begin{align}
    \delta_{kj} = \begin{cases}
        1 & \text{, if } k = j \\
        0 & \text{, else}
    \end{cases}
\end{align}

A linear map from a complex vector space to itself is represented as a \textbf{square} matrix.
The following two concepts only exist for this kind of map.
The \textbf{trace} of a square matrix $\Tr: \C^{n \times n} \to \C$ is defined as
\begin{align}
    \Tr(A) = \sum_i a_{ii}
\end{align}

For a linear map $A$ from a complex vector space $\C^n$ to itself, there is at least one vector $v \in \C^n$ and a scalar $\lambda \in \C$ so that
\begin{align}
    A \cdot v & = \lambda \cdot v
\end{align}
$v$ is called \textbf{Eigenvector} of $A$ and $\lambda$ is called \textbf{Eigenvalue} of $A$.
The set of all Eigenvalues of a matrix is denoted $\sigma_A = \left\{ \lambda_i \right\}$.
If $v_i \in \C^n$ is an Eigenvector with the Eigenvalue $\lambda_i$, then every multiple $\kappa \cdot v_i$ with $\kappa \in \C$ is also Eigenvector of $A$ with the Eigenvalue $\lambda_i$.
There may be other vectors that also satisfy the condition for the same $\lambda_i$.
All the vectors, that do form a vector subspace denoted
\begin{align}
    E_{\lambda_i} & = \left\{ v \in \C^n | A \cdot v = \lambda_i \cdot v \right\}
\end{align}

%% start needed?
%\todo[inline]{Is scalar product nescessary?}

%On $\C^n$, the \textbf{scalar product} (also called dot product) $\langle \cdot, \cdot \rangle: \C^n \times \C^n \to \C$ is defined as
%\begin{align}
%    \langle v, w \rangle = \sum_i v_i \cdot w_i
%\end{align}
%It can also be thought of as a matrix multiplication of a row vector and a column vector $\langle v, w \rangle = v^T w$.
%% end needed?

The \textbf{direct sum} of two matrices $\oplus: \C^{a \times b} \times \C^{c \times d} \to \C^{(a + c) \times (b + d)}$ is defined as
\begin{align}
    A \oplus B = \begin{pmatrix}
        A & 0 \\
        0 & B
    \end{pmatrix}
\end{align}
where each zero stands for a matrix that only has zeros.
Matrices, that can be written as the direct sum of two or more matrices are called \textbf{block matrices}.

\todo[inline]{look at it again}
The \textbf{tensor product} of two matrices $\otimes: \C^{a \times b} \times \C^{c \times d} \to \C^{(a \cdot c) \times (b \cdot d)}$ is defined as
\begin{align}
    A \otimes B = \begin{pmatrix}
        a_{1, 1} \cdot B & a_{1, 2} \cdot B & \ldots & a_{1, b} \cdot B \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{a, 1} \cdot B & a_{a, 2} \cdot B & \ldots & a_{a, b} \cdot B
    \end{pmatrix}
\end{align}

\todo[inline]{add complementary vector spaces}

%% start needed?
%\todo[inline]{outer and symmetric powers}

%\todo[inline]{Is change of basis necessary?}

%A \textbf{change of basis} from one basis $\{v_i\}_i$ to another one $\{w_j\}_j$ in a vector space $C^n$ can be represented by an invertible matrix.
%We find this matrix by writing each vector of the new basis $w_j$ as a linear combination of the vectors in the old basis $v_i$.
%\begin{align}
%    w_j = \sum_i a_{ij} v_i
%\end{align}
%This gives us the elements of $A = (a_{ij})_{ij}$.
%A vector $x \in \C^n$ is transformed from the old to the new basis via a matrix multiplication $x' = A^{-1} \cdot x$.
%A linear map between $\C^n$ and itself represented as a matrix $M \in \C^{n \times n}$ is transformed via two matrix multiplications $M' = A^{-1} \cdot M \cdot A$.
%\\
%
%\todo[inline]{still necessary? Do I introduce dual representation later?}
%
%A \textbf{dual vector space} $V^*$ of a vector space $V$ over a field $F$ is the set of all linear maps $m: V \to F$.
%In the case of $\C^n = \C^{n \times 1}$ we can think of it as row vectors $(\C^n)^* = \C^{1 \times n}$.
%Multiplying a row vector with a column vector will result in a number, this follows from the definition of matrix multiplication above.
%Since every element in the row vector corresponds to the weight of one element in the column vector, before summing everything up, every linear map $m: \C^n \to \C$ can be represented by exactly one row vector.
